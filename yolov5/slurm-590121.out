[34m[1mhyperparameters: [0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
[34m[1mTensorBoard: [0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/
Overriding model.yaml nc=80 with nc=1

                 from  n    params  module                                  arguments                     
  0                -1  1      4672  models.common.Conv                      [4, 32, 6, 2, 2]              
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                
  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               
  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              
  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              
  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 
  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 12           [-1, 6]  1         0  models.common.Concat                    [1]                           
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 16           [-1, 4]  1         0  models.common.Concat                    [1]                           
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              
 19          [-1, 14]  1         0  models.common.Concat                    [1]                           
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              
 22          [-1, 10]  1         0  models.common.Concat                    [1]                           
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
Model Summary: 270 layers, 7023478 parameters, 7023478 gradients

Transferred 342/349 items from yolov5s.pt
Scaled weight_decay = 0.0005
[34m[1moptimizer:[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias
[34m[1malbumentations: [0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))
[34m[1malbumentations: [0mChannelShuffle(always_apply=False, p=1), Blur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))
[34m[1mtrain: [0mweights=yolov5s.pt, cfg=, data=data/LLVIP.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=5, batch_size=8, imgsz=1280, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=LLVIP_export, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest
[34m[1mgithub: [0mskipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5
[31m[1mrequirements:[0m torchvision>=0.8.1 not found and is required by YOLOv5, attempting auto-update...
[31m[1mrequirements:[0m 'pip install torchvision>=0.8.1' skipped (offline)
[34m[1mWeights & Biases: [0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)
[34m[1mtrain: [0mScanning '/home/avvoronova/diploma/dataset/labels/train_inf.cache' images and labels... 10821 found, 1 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10822/10822 [00:00<?, ?it/s][34m[1mtrain: [0mScanning '/home/avvoronova/diploma/dataset/labels/train_inf.cache' images and labels... 10821 found, 1 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10822/10822 [00:00<00:00, 152830834.64it/s]
[34m[1mval: [0mScanning '/home/avvoronova/diploma/dataset/labels/val_inf.cache' images and labels... 1203 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1203/1203 [00:00<?, ?it/s][34m[1mval: [0mScanning '/home/avvoronova/diploma/dataset/labels/val_inf.cache' images and labels... 1203 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1203/1203 [00:00<00:00, 12427950.03it/s]
Image sizes 1280 train, 1280 val
Using 8 dataloader workers
Logging results to [1mruns/train/LLVIP_export42[0m
Starting training for 5 epochs...

     Epoch   gpu_mem       box       obj       cls    labels  img_size
  0%|          | 0/1353 [00:00<?, ?it/s]       0/4     5.99G    0.1234    0.1125         0        51      1280:   0%|          | 0/1353 [00:00<?, ?it/s]       0/4     5.99G    0.1234    0.1125         0        51      1280:   0%|          | 1/1353 [00:03<1:22:14,  3.65s/it]       0/4        6G    0.1254    0.1086         0        39      1280:   0%|          | 1/1353 [00:04<1:22:14,  3.65s/it]       0/4        6G    0.1254    0.1086         0        39      1280:   0%|          | 2/1353 [00:04<1:01:52,  2.75s/it]       0/4     6.64G    0.1221    0.1073         0        38      1280:   0%|          | 2/1353 [00:05<1:01:52,  2.75s/it]       0/4     6.64G    0.1221    0.1073         0        38      1280:   0%|          | 3/1353 [00:05<49:59,  2.22s/it]         0/4     6.64G    0.1217    0.1076         0        49      1280:   0%|          | 3/1353 [00:06<49:59,  2.22s/it]       0/4     6.64G    0.1217    0.1076         0        49      1280:   0%|          | 4/1353 [00:06<42:41,  1.90s/it]       0/4  Plotting labels... 

[34m[1mautoanchor: [0mAnalyzing anchors... anchors/target = 4.29, Best Possible Recall (BPR) = 0.9998
   6.64G    0.1214     0.108         0        60      1280:   0%|          | 4/1353 [00:07<42:41,  1.90s/it]       0/4     6.64G    0.1214     0.108         0        60      1280:   0%|          | 5/1353 [00:07<34:57,  1.56s/it]       0/4     6.64G    0.1204    0.1069         0        46      1280:   0%|          | 5/1353 [00:07<34:57,  1.56s/it]       0/4     6.64G    0.1204    0.1069         0        46      1280:   0%|          | 6/1353 [00:07<29:42,  1.32s/it]       0/4     6.64G    0.1189    0.1051         0        39      1280:   0%|          | 6/1353 [00:08<29:42,  1.32s/it]       0/4     6.64G    0.1189    0.1051         0        39      1280:   1%|          | 7/1353 [00:08<26:26,  1.18s/it]       0/4     6.64G    0.1197    0.1028         0        30      1280:   1%|          | 7/1353 [00:09<26:26,  1.18s/it]       0/4     6.64G    0.1197    0.1028         0        30      1280:   1%|          | 8/1353 [00:09<25:05,  1.12s/it]Traceback (most recent call last):
  File "train.py", line 652, in <module>
    main(opt)
  File "train.py", line 548, in main
    train(opt.hyp, opt, device, callbacks)
  File "train.py", line 294, in train
    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------
  File "/home/avvoronova/.conda/envs/diploma/lib/python3.7/site-packages/tqdm/std.py", line 1104, in __iter__
    for obj in iterable:
  File "/home/avvoronova/diploma/LLVIP/yolov5/utils/datasets.py", line 140, in __iter__
    yield next(self.iterator)
  File "/home/avvoronova/.conda/envs/diploma/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/home/avvoronova/.conda/envs/diploma/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1085, in _next_data
    return self._process_data(data)
  File "/home/avvoronova/.conda/envs/diploma/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1111, in _process_data
    data.reraise()
  File "/home/avvoronova/.conda/envs/diploma/lib/python3.7/site-packages/torch/_utils.py", line 428, in reraise
    raise self.exc_type(msg)
IndexError: Caught IndexError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/avvoronova/.conda/envs/diploma/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 198, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/avvoronova/.conda/envs/diploma/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/avvoronova/.conda/envs/diploma/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/avvoronova/diploma/LLVIP/yolov5/utils/datasets.py", line 620, in __getitem__
    img, labels = self.albumentations2(img, labels)
  File "/home/avvoronova/diploma/LLVIP/yolov5/utils/augmentations.py", line 78, in __call__
    new = self.transform(image=im, bboxes=labels[:, 1:], class_labels=labels[:, 0])  # transformed
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed

       0/4     6.64G    0.1197    0.1028         0        30      1280:   1%|          | 8/1353 [00:10<30:10,  1.35s/it]srun: error: cn-006: task 0: Exited with exit code 1
